# Python-Developer-Data-Engineer
Preparaci√≥n para el puesto de Python Developer / Data Engineer en Grupo Oricteropo Tropical. Plan enfocado en los requisitos y responsabilidades clave de la oferta.

## Oferta
## **Python Developer / Data Engineer**
*Grupo Oricteropo Tropical* 
‚Ä¢ Aguadilla ‚Ä¢ via Ladders
‚Ä¢ $60,608‚Äì$101,014 a year
‚Ä¢ Full-time
‚Ä¢ Apply on Ladders

**Job description**
Who We Are

Grupo Oricteropo Tropical (GO Tropical) is a Puerto Rican Act 20 company, established to provide services such as capital allocation, risk management, software development, and centralized management ("headquarters" for TransMarket Group) to its international affiliates and customers. We have the intensity and passion of a technology startup while maintaining our stability and storied history as a respected member of the global financial system for over 40 years.

Our mission is to bless others through the services we provide and through the generous stewardship of the wealth we create. The impact we make with the work we do drives our humility, discipline, and pursuit of opportunity.

GO Tropical is excited to provide a unique employment opportunity with our rapidly growing team in Aguadilla, Puerto Rico. Our long-term employee training and development proves we value our people and want them to succeed at all levels in their careers. We have an entrepreneurial culture and collaboratively develop our business with patience and discipline; we work hard, learn constantly, and relentlessly improve our expertise.

**Responsibilities**
‚Ä¢ Support and enhance the firm's non-latency systems and data mart. This hands-on role will collaborate with all departments at TransMarket Group to complete goals in the following areas: Finance, risk, accounting, compliance, clearing, operations, portfolio financing.
‚Ä¢ Design, implement, and support software solutions that provide a unique insight into the firm's operations
‚Ä¢ Work with various teams across the firm to understand system requirements and needs
‚Ä¢ Quickly respond to ad hoc data and reporting requests
‚Ä¢ Enhance existing surveillance and alerting infrastructure

**Requirements**
‚Ä¢ BA/BS/MS Computer Science, Engineering, Information Technology or related industry experience
‚Ä¢ Strong knowledge of Python required; Pandas is a plus
‚Ä¢ Database design and support is a plus, basic SQL required
‚Ä¢ Knowledge of RESTful API design
‚Ä¢ Solid understanding of data structures & containers
‚Ä¢ Basic Linux and source control (git) understanding
‚Ä¢ Familiarity with data warehousing and mining
‚Ä¢ Dashboard design and data visualization a plus
‚Ä¢ This position requires physical presence and is onsite (no remote work)

**Benefits**

We offer one of the most generous profit sharing programs in the industry because we believe our employees should be able to take part in our rapid growth and success. We are proud to offer more world-class benefits for our employees and their families.

Grupo Oricteropo Tropical (GO Tropical) is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, gender, sexual orientation, gender identity or expression, religion, national origin, marital status, age, disability, veteran status, genetic information, or any other protected status
---
Aqu√≠ tienes un **plan intensivo de 5 d√≠as (40-48 horas)** para profundizar en **Modelos de Datos** y **Miner√≠a de Datos**, con enfoque en **estructuras de almacenamiento**, **algoritmos de miner√≠a de datos** y su aplicaci√≥n en **Python** usando librer√≠as como **Pandas**, **Scikit-learn**, **SQL**, y **NoSQL**.  

---
## **Semana 1**
**D√≠a 1 (8-10 horas) - Fundamentos de Modelos de Datos y Estructuras de Almacenamiento**  
üìå **Objetivo:** Entender los **modelos de datos** y las **estructuras de almacenamiento** m√°s comunes.  

### **Conceptos Clave:**  
‚úÖ Modelos de datos: **Relacional**, **NoSQL**, **Jer√°rquico**, **Red**.  
‚úÖ Bases de datos relacionales (SQL) y NoSQL (MongoDB, Redis).  
‚úÖ Diferencias entre almacenamiento en **columnas** y **filas**.  
‚úÖ Principales arquitecturas de almacenamiento: **Data Warehouses**, **Data Lakes**.  
‚úÖ Introducci√≥n a **ETL** (Extract, Transform, Load) y **ELT**.  

üìå **Pr√°ctica:**  
- Crear y consultar bases de datos **relacionales** (SQLite, MySQL).  
- Configurar un **servidor NoSQL** (MongoDB).  
- Realizar operaciones de **insert, update, delete, select** en SQL.  
- Implementar un proceso **ETL** b√°sico utilizando **Pandas**.  

**Recursos:**  
- [Database Normalization](https://www.guru99.com/database-normalization.html).  
- [MongoDB Docs](https://www.mongodb.com/docs/).  
- [ETL with Python](https://realpython.com/python-etl/).  

---

## **D√≠a 2 (8-10 horas) - Introducci√≥n a la Miner√≠a de Datos**  
üìå **Objetivo:** Familiarizarte con los **conceptos y algoritmos** fundamentales de la miner√≠a de datos.  

### **Conceptos Clave:**  
‚úÖ Definici√≥n de **Miner√≠a de Datos** y su relaci√≥n con el an√°lisis de grandes vol√∫menes de datos.  
‚úÖ T√©cnicas de miner√≠a de datos: **Clasificaci√≥n**, **Regresi√≥n**, **Clustering**, **Asociaci√≥n**.  
‚úÖ Principales algoritmos: **K-means**, **Decision Trees**, **Random Forests**, **Apriori**, **KNN**.  
‚úÖ Preprocesamiento de datos: **limpieza**, **normalizaci√≥n**, **transformaci√≥n**.  

üìå **Pr√°ctica:**  
- Aplicar el algoritmo **K-means** para clustering en un dataset.  
- Implementar un **√Årbol de Decisi√≥n** usando **Scikit-learn** para clasificaci√≥n.  
- Realizar un an√°lisis de **asociaci√≥n** utilizando **Apriori**.  
- Preprocesar un conjunto de datos (eliminar nulos, normalizar) con **Pandas**.  

**Recursos:**  
- [Mining of Massive Datasets](http://www.mmds.org/).  
- [Scikit-learn Docs](https://scikit-learn.org/stable/).  
- [Data Preprocessing Techniques](https://towardsdatascience.com/data-preprocessing-techniques-you-should-know-24eaf3c69f4).  

---

## **D√≠a 3 (8-10 horas) - Modelado Predictivo y Algoritmos de Aprendizaje Autom√°tico**  
üìå **Objetivo:** Profundizar en modelos predictivos y algoritmos avanzados de **aprendizaje autom√°tico** aplicados a miner√≠a de datos.  

### **Conceptos Clave:**  
‚úÖ **Modelos supervisados**: **Regresi√≥n lineal**, **SVM**, **Redes neuronales**.  
‚úÖ **Modelos no supervisados**: **PCA**, **Clustering jer√°rquico**, **DBSCAN**.  
‚úÖ Validaci√≥n de modelos: **Cross-validation**, **Grid Search**, **Hyperparameter tuning**.  
‚úÖ M√©tricas de rendimiento: **Accuracy**, **Precision**, **Recall**, **F1-score**, **ROC-AUC**.  

üìå **Pr√°ctica:**  
- Implementar un modelo de **Regresi√≥n Lineal** y evaluar su rendimiento.  
- Aplicar un modelo de **SVM** para clasificaci√≥n.  
- Realizar **PCA** para reducci√≥n de dimensionalidad.  
- Ajustar hiperpar√°metros de un **Random Forest** con **GridSearchCV**.  

**Recursos:**  
- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).  
- [Scikit-learn: Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html).  
- [Kaggle Datasets](https://www.kaggle.com/datasets).  

---

## **D√≠a 4 (8-10 horas) - T√©cnicas Avanzadas en Miner√≠a de Datos y Evaluaci√≥n de Modelos**  
üìå **Objetivo:** Profundizar en **t√©cnicas avanzadas** de miner√≠a de datos y **evaluaci√≥n de modelos**.  

### **Conceptos Clave:**  
‚úÖ Algoritmos avanzados: **XGBoost**, **LightGBM**, **CatBoost**.  
‚úÖ T√©cnicas de **ensembles**: **Bagging**, **Boosting**, **Stacking**.  
‚úÖ T√©cnicas de **overfitting** y **underfitting**.  
‚úÖ Interpretaci√≥n de resultados con **SHAP**, **LIME**, y **feature importance**.  
‚úÖ Evaluaci√≥n avanzada: **Confusion Matrix**, **Precision-Recall Curve**, **Learning Curve**.  

üìå **Pr√°ctica:**  
- Implementar un modelo de **XGBoost** y **LightGBM** para un problema de clasificaci√≥n.  
- Aplicar **Cross-validation** y ajustar modelos para evitar el **overfitting**.  
- Evaluar un modelo con **Confusion Matrix** y **Precision-Recall Curve**.  
- Interpretar un modelo usando **SHAP** o **LIME** para obtener explicaciones sobre predicciones.  

**Recursos:**  
- [XGBoost Docs](https://xgboost.readthedocs.io/).  
- [LightGBM Docs](https://lightgbm.readthedocs.io/).  
- [SHAP Library](https://github.com/slundberg/shap).  

---

## **D√≠a 5 (8-10 horas) - Aplicaci√≥n Pr√°ctica: Proyecto de Miner√≠a de Datos**  
üìå **Objetivo:** Aplicar todo lo aprendido en un **proyecto de miner√≠a de datos** desde la recolecci√≥n hasta la evaluaci√≥n del modelo.  

### **Conceptos Clave:**  
‚úÖ Definir el **problema de negocio**.  
‚úÖ Selecci√≥n y limpieza de **datasets**.  
‚úÖ Desarrollo de **modelos predictivos** y su validaci√≥n.  
‚úÖ Comunicaci√≥n de resultados a trav√©s de **visualizaciones** y **reportes**.  

üìå **Pr√°ctica:**  
- Elegir un conjunto de datos real (de Kaggle o un repositorio p√∫blico).  
- Aplicar t√©cnicas de **preprocesamiento** y **miner√≠a de datos** para resolver un problema de negocio.  
- Desarrollar un **modelo predictivo** y realizar su evaluaci√≥n.  
- Presentar los resultados en un **informe** con visualizaciones y m√©tricas clave.  

**Recursos:**  
- [Kaggle Competitions](https://www.kaggle.com/competitions).  
- [Machine Learning Mastery](https://machinelearningmastery.com/).  

---
## **Semana 2**
**D√≠a 1 (8-10 horas) - Fundamentos de SQL y Dise√±o de Bases de Datos**  
üìå **Objetivo:** Comprender los conceptos fundamentales de bases de datos relacionales y el dise√±o eficiente de tablas.  

### **Conceptos Clave:**  
‚úÖ ¬øQu√© es una base de datos relacional?  
‚úÖ Modelo entidad-relaci√≥n (ERD).  
‚úÖ Normalizaci√≥n (1FN, 2FN, 3FN).  
‚úÖ Tipos de datos en SQL (`INT`, `VARCHAR`, `DATE`, `BOOLEAN`, `DECIMAL`).  
‚úÖ Creaci√≥n de tablas y relaciones (`CREATE TABLE`, `PRIMARY KEY`, `FOREIGN KEY`).  

üìå **Pr√°ctica:**  
- Dise√±ar un modelo ERD para un sistema de ventas (clientes, productos, √≥rdenes, pagos).  
- Crear la base de datos en **PostgreSQL** o **MySQL** y definir tablas correctamente.  

**Recursos:**  
- [SQLBolt (curso interactivo)](https://sqlbolt.com/)  
- "Database Design for Mere Mortals" - Michael Hernandez  

---

## **D√≠a 2 (8-10 horas) - Consultas SQL B√°sicas y Agregaciones**  
üìå **Objetivo:** Dominar las consultas fundamentales para extraer y analizar datos.  

### **Consultas Esenciales:**  
‚úÖ `SELECT`, `WHERE`, `ORDER BY`, `LIMIT`, `DISTINCT`.  
‚úÖ Filtros con operadores (`=`, `!=`, `<`, `>`, `LIKE`, `IN`, `BETWEEN`).  
‚úÖ Agregaciones (`COUNT()`, `SUM()`, `AVG()`, `MIN()`, `MAX()`).  
‚úÖ Agrupaciones con `GROUP BY` y `HAVING`.  

üìå **Pr√°ctica:**  
- Consultar un dataset de clientes y analizar compras por usuario.  
- Calcular promedios de ventas por mes.  

**Recursos:**  
- "SQL for Data Scientists" - Renee M. P. Teate.  
- [Mode Analytics SQL Tutorial](https://mode.com/sql-tutorial/).  

---

## **D√≠a 3 (8-10 horas) - Consultas Avanzadas y Optimizaci√≥n**  
üìå **Objetivo:** Aprender a trabajar con m√∫ltiples tablas y optimizar consultas.  

### **Conceptos Clave:**  
‚úÖ `JOINs` (`INNER JOIN`, `LEFT JOIN`, `RIGHT JOIN`, `FULL JOIN`).  
‚úÖ Subconsultas (`SELECT dentro de SELECT`).  
‚úÖ Funciones de ventana (`RANK()`, `DENSE_RANK()`, `ROW_NUMBER()`, `LAG()`, `LEAD()`).  
‚úÖ Indexaci√≥n (`CREATE INDEX` para mejorar rendimiento).  

üìå **Pr√°ctica:**  
- Obtener la lista de clientes con sus √≥rdenes y total gastado.  
- Calcular el ranking de productos m√°s vendidos usando `RANK()`.  
- Crear √≠ndices en una tabla con millones de registros y medir el tiempo de ejecuci√≥n.  

**Recursos:**  
- [LeetCode SQL Challenges](https://leetcode.com/problemset/database/).  
- Documentaci√≥n oficial de PostgreSQL/MySQL.  

---

## **D√≠a 4 (8-10 horas) - Integraci√≥n de SQL con Python (Pandas y SQLAlchemy)**  
üìå **Objetivo:** Aprender a conectar bases de datos con Python para extraer y analizar datos.  

### **Conceptos Clave:**  
‚úÖ Conectar SQL con Python usando `SQLAlchemy` y `sqlite3`.  
‚úÖ Extraer datos en un **DataFrame de Pandas** con `pd.read_sql_query()`.  
‚úÖ Escribir datos en SQL desde Pandas con `.to_sql()`.  
‚úÖ Automatizaci√≥n de consultas con Python.  

üìå **Pr√°ctica:**  
- Escribir un script en Python que extraiga datos de SQL y genere un an√°lisis con Pandas.  
- Crear un pipeline de ETL que lea, transforme y guarde datos en una base de datos SQL.  

**Recursos:**  
- "Python for Data Analysis" - Wes McKinney.  
- [SQLAlchemy Docs](https://www.sqlalchemy.org/).  

---

## **D√≠a 5 (6-8 horas) - Proyecto Final: An√°lisis de Datos con SQL y Pandas**  
üìå **Objetivo:** Aplicar todo lo aprendido en un caso real.  

‚úÖ **Proyecto:**  
- Extraer datos de una base de datos SQL sobre transacciones de un e-commerce.  
- Limpiar y analizar datos en Pandas.  
- Crear un dashboard en **Streamlit** o **Dash** para visualizar insights.  
- Generar reportes autom√°ticos con Python y SQL.  

üìå **Entrega:**  
- Subir c√≥digo a **GitHub** con README y documentaci√≥n.    

---

## **D√≠a 1 (8-10 horas) - Fundamentos de Linux y Terminal**  
üìå **Objetivo:** Aprender a navegar y manipular archivos en Linux usando la terminal.  

### **Conceptos Clave:**  
‚úÖ Navegaci√≥n b√°sica: `ls`, `cd`, `pwd`, `tree`.  
‚úÖ Manipulaci√≥n de archivos: `cp`, `mv`, `rm`, `touch`, `mkdir`, `find`.  
‚úÖ Permisos y usuarios: `chmod`, `chown`, `sudo`.  
‚úÖ Uso de `grep`, `sed`, `awk` para manipulaci√≥n de texto.  
‚úÖ Procesos y administraci√≥n del sistema: `ps`, `top`, `kill`, `df`, `du`, `htop`.  

üìå **Pr√°ctica:**  
- Crear y organizar un directorio con m√∫ltiples archivos usando comandos.  
- Buscar l√≠neas espec√≠ficas en un archivo de logs usando `grep` y `awk`.  
- Cambiar permisos de archivos y probar con diferentes usuarios.  

**Recursos:**  
- [Linux Journey](https://linuxjourney.com/).  
- [Explainshell](https://explainshell.com/) (para entender comandos).  

---

## **D√≠a 2 (8-10 horas) - Shell Scripting y Automatizaci√≥n en Linux**  
üìå **Objetivo:** Aprender a escribir scripts en Bash para automatizar tareas.  

### **Conceptos Clave:**  
‚úÖ Variables y argumentos en Bash (`$1`, `$2`, `$@`).  
‚úÖ Estructuras de control (`if`, `for`, `while`).  
‚úÖ Redirecci√≥n de salida y pipes (`>`, `>>`, `|`).  
‚úÖ Tareas programadas con `cron` y `crontab`.  
‚úÖ Gesti√≥n de logs y monitoreo de procesos.  

üìå **Pr√°ctica:**  
- Escribir un script para **automatizar limpieza de logs**.  
- Crear un **cron job** para respaldar una carpeta diariamente.  
- Construir un script que monitoree el uso de CPU y RAM.  

**Recursos:**  
- [Bash Scripting Tutorial](https://ryanstutorials.net/bash-scripting-tutorial/).  
- [Cron Job Generator](https://crontab.guru/).  

---

## **D√≠a 3 (8-10 horas) - Fundamentos de Git y Control de Versiones**  
üìå **Objetivo:** Aprender a usar **Git** para trabajar en proyectos colaborativos.  

### **Conceptos Clave:**  
‚úÖ Inicializar y clonar repositorios (`git init`, `git clone`).  
‚úÖ Rastrear cambios (`git add`, `git commit`).  
‚úÖ Historial y diferencias (`git log`, `git diff`).  
‚úÖ Ramas (`git branch`, `git checkout`, `git merge`).  
‚úÖ Manejo de conflictos en `git merge`.  

üìå **Pr√°ctica:**  
- Crear un **repositorio en GitHub** y hacer commits con cambios.  
- Trabajar con **ramas** y fusionarlas.  
- Resolver un conflicto de merge en un archivo de c√≥digo.  

**Recursos:**  
- [Learn Git Branching](https://learngitbranching.js.org/).  
- [GitHub Docs](https://docs.github.com/en/get-started).  

---

## **D√≠a 4 (6-8 horas) - Trabajo Avanzado con Git y GitHub**  
üìå **Objetivo:** Aprender a colaborar en equipos y usar herramientas avanzadas de Git.  

### **Conceptos Clave:**  
‚úÖ Forks y Pull Requests (`git fork`, `git pull request`).  
‚úÖ Rebases y commits limpios (`git rebase`, `git squash`).  
‚úÖ Uso de `.gitignore` para evitar archivos innecesarios.  
‚úÖ Configuraci√≥n de llaves SSH para autenticaci√≥n en GitHub.  

üìå **Pr√°ctica:**  
- Contribuir a un **proyecto open-source** en GitHub.  
- Hacer un **fork**, modificar c√≥digo y enviar un **Pull Request**.  
- Configurar **GitHub Actions** para automatizar pruebas en un proyecto.  

**Recursos:**  
- [Pro Git Book](https://git-scm.com/book/en/v2).  
- [GitHub Actions Docs](https://docs.github.com/en/actions).  

---
## **Semana 3**
**D√≠a 1 (8-10 horas) - Fundamentos de Visualizaci√≥n de Datos**  
üìå **Objetivo:** Comprender los principios fundamentales del dise√±o de visualizaciones efectivas.  

### **Conceptos Clave:**  
‚úÖ Tipos de gr√°ficos: Barras, L√≠neas, Dispersi√≥n, Histogramas, Boxplots.  
‚úÖ Uso adecuado de colores, etiquetas y escalas.  
‚úÖ Importancia del contexto y narraci√≥n de datos.  

üìå **Pr√°ctica:**  
- Analizar un dataset con Pandas (`.describe()`, `.info()`).  
- Graficar distribuciones con Matplotlib y Seaborn.  
- Crear histogramas y boxplots para detectar **outliers**.  

**Recursos:**  
- "Storytelling with Data" - Cole Nussbaumer Knaflic.  
- [Data-to-Viz](https://www.data-to-viz.com/) (gu√≠a interactiva de gr√°ficos).  

---

## **D√≠a 2 (8-10 horas) - Visualizaci√≥n con Matplotlib y Seaborn**  
üìå **Objetivo:** Aprender a usar **Matplotlib** y **Seaborn** para generar visualizaciones detalladas.  

### **Conceptos Clave:**  
‚úÖ `plt.figure()`, `plt.subplot()` para gr√°ficos personalizados.  
‚úÖ `sns.pairplot()`, `sns.heatmap()` para correlaciones.  
‚úÖ Personalizaci√≥n avanzada (`colors`, `ticks`, `labels`).  

üìå **Pr√°ctica:**  
- Comparar ventas anuales con gr√°ficos de barras y l√≠neas.  
- Crear mapas de calor de correlaciones entre variables.  
- Ajustar tama√±os, colores y t√≠tulos para gr√°ficos m√°s profesionales.  

**Recursos:**  
- [Matplotlib Docs](https://matplotlib.org/stable/contents.html).  
- [Seaborn Docs](https://seaborn.pydata.org/tutorial.html).  

---

## **D√≠a 3 (8-10 horas) - Visualizaci√≥n Interactiva con Plotly**  
üìå **Objetivo:** Generar gr√°ficos interactivos con **Plotly Express**.  

### **Conceptos Clave:**  
‚úÖ `px.scatter()`, `px.bar()`, `px.line()`, `px.box()`.  
‚úÖ Dashboards con `plotly.graph_objects`.  
‚úÖ Uso de sliders y filtros din√°micos.  

üìå **Pr√°ctica:**  
- Crear gr√°ficos interactivos para an√°lisis de datos financieros.  
- Construir un dashboard con m√∫ltiples gr√°ficos en una sola vista.  

**Recursos:**  
- [Plotly Express Docs](https://plotly.com/python/plotly-express/).  

---

## **D√≠a 4 (8-10 horas) - Creaci√≥n de Dashboards con Streamlit**  
üìå **Objetivo:** Construir dashboards interactivos con **Streamlit**.  

### **Conceptos Clave:**  
‚úÖ `st.line_chart()`, `st.bar_chart()`, `st.map()`.  
‚úÖ Widgets interactivos (`st.slider()`, `st.selectbox()`, `st.button()`).  
‚úÖ Integraci√≥n con Pandas y bases de datos SQL.  

üìå **Pr√°ctica:**  
- Crear un dashboard para seguimiento de precios de criptomonedas.  
- Implementar filtros din√°micos y visualizaciones interactivas.  

**Recursos:**  
- [Streamlit Docs](https://docs.streamlit.io/).  

---

## **D√≠a 5 (8-10 horas) - Fundamentos de Power BI**  
üìå **Objetivo:** Familiarizarse con **Power BI** para crear visualizaciones avanzadas.  

### **Conceptos Clave:**  
‚úÖ Cargar datos desde fuentes diversas (Excel, SQL, Web).  
‚úÖ Creaci√≥n de gr√°ficos y tablas din√°micas.  
‚úÖ Uso de medidas, KPIs y filtros.  
‚úÖ Personalizaci√≥n de informes.  

üìå **Pr√°ctica:**  
- Importar un conjunto de datos desde Excel y crear gr√°ficos b√°sicos.  
- Crear un dashboard b√°sico con KPIs y filtros interactivos.  
- Aplicar formateo condicional a tablas y visualizaciones.  

**Recursos:**  
- [Power BI Docs](https://docs.microsoft.com/en-us/power-bi/).  
- [Power BI Tutorial for Beginners](https://www.sqlshack.com/learn-power-bi/).  

---

## **D√≠a 6 (8-10 horas) - Fundamentos de Tableau y Proyecto Final**  
üìå **Objetivo:** Familiarizarse con **Tableau** y crear un dashboard interactivo.  

### **Conceptos Clave:**  
‚úÖ Cargar y limpiar datos en Tableau.  
‚úÖ Crear gr√°ficos interactivos y mapas.  
‚úÖ Uso de filtros, acciones y par√°metros.  
‚úÖ Publicar dashboards en Tableau Public.  

üìå **Pr√°ctica:**  
- Conectar a un conjunto de datos y crear un gr√°fico de dispersi√≥n.  
- Dise√±ar un dashboard interactivo con filtros y gr√°ficos combinados.  
- Publicar el dashboard en **Tableau Public**.  

**Recursos:**  
- [Tableau Docs](https://help.tableau.com/current/guides.htm).  
- [Tableau Public Gallery](https://public.tableau.com/app/profile).  

---
## **Semana 4**
**D√≠a 1 (8-10 horas) - Fundamentos de Modelos de Datos y Estructuras de Almacenamiento**  
üìå **Objetivo:** Entender los **modelos de datos** y las **estructuras de almacenamiento** m√°s comunes.  

### **Conceptos Clave:**  
‚úÖ Modelos de datos: **Relacional**, **NoSQL**, **Jer√°rquico**, **Red**.  
‚úÖ Bases de datos relacionales (SQL) y NoSQL (MongoDB, Redis).  
‚úÖ Diferencias entre almacenamiento en **columnas** y **filas**.  
‚úÖ Principales arquitecturas de almacenamiento: **Data Warehouses**, **Data Lakes**.  
‚úÖ Introducci√≥n a **ETL** (Extract, Transform, Load) y **ELT**.  

üìå **Pr√°ctica:**  
- Crear y consultar bases de datos **relacionales** (SQLite, MySQL).  
- Configurar un **servidor NoSQL** (MongoDB).  
- Realizar operaciones de **insert, update, delete, select** en SQL.  
- Implementar un proceso **ETL** b√°sico utilizando **Pandas**.  

**Recursos:**  
- [Database Normalization](https://www.guru99.com/database-normalization.html).  
- [MongoDB Docs](https://www.mongodb.com/docs/).  
- [ETL with Python](https://realpython.com/python-etl/).  

---

## **D√≠a 2 (8-10 horas) - Introducci√≥n a la Miner√≠a de Datos**  
üìå **Objetivo:** Familiarizarte con los **conceptos y algoritmos** fundamentales de la miner√≠a de datos.  

### **Conceptos Clave:**  
‚úÖ Definici√≥n de **Miner√≠a de Datos** y su relaci√≥n con el an√°lisis de grandes vol√∫menes de datos.  
‚úÖ T√©cnicas de miner√≠a de datos: **Clasificaci√≥n**, **Regresi√≥n**, **Clustering**, **Asociaci√≥n**.  
‚úÖ Principales algoritmos: **K-means**, **Decision Trees**, **Random Forests**, **Apriori**, **KNN**.  
‚úÖ Preprocesamiento de datos: **limpieza**, **normalizaci√≥n**, **transformaci√≥n**.  

üìå **Pr√°ctica:**  
- Aplicar el algoritmo **K-means** para clustering en un dataset.  
- Implementar un **√Årbol de Decisi√≥n** usando **Scikit-learn** para clasificaci√≥n.  
- Realizar un an√°lisis de **asociaci√≥n** utilizando **Apriori**.  
- Preprocesar un conjunto de datos (eliminar nulos, normalizar) con **Pandas**.  

**Recursos:**  
- [Mining of Massive Datasets](http://www.mmds.org/).  
- [Scikit-learn Docs](https://scikit-learn.org/stable/).  
- [Data Preprocessing Techniques](https://towardsdatascience.com/data-preprocessing-techniques-you-should-know-24eaf3c69f4).  

---

## **D√≠a 3 (8-10 horas) - Modelado Predictivo y Algoritmos de Aprendizaje Autom√°tico**  
üìå **Objetivo:** Profundizar en modelos predictivos y algoritmos avanzados de **aprendizaje autom√°tico** aplicados a miner√≠a de datos.  

### **Conceptos Clave:**  
‚úÖ **Modelos supervisados**: **Regresi√≥n lineal**, **SVM**, **Redes neuronales**.  
‚úÖ **Modelos no supervisados**: **PCA**, **Clustering jer√°rquico**, **DBSCAN**.  
‚úÖ Validaci√≥n de modelos: **Cross-validation**, **Grid Search**, **Hyperparameter tuning**.  
‚úÖ M√©tricas de rendimiento: **Accuracy**, **Precision**, **Recall**, **F1-score**, **ROC-AUC**.  

üìå **Pr√°ctica:**  
- Implementar un modelo de **Regresi√≥n Lineal** y evaluar su rendimiento.  
- Aplicar un modelo de **SVM** para clasificaci√≥n.  
- Realizar **PCA** para reducci√≥n de dimensionalidad.  
- Ajustar hiperpar√°metros de un **Random Forest** con **GridSearchCV**.  

**Recursos:**  
- [Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow](https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/).  
- [Scikit-learn: Supervised Learning](https://scikit-learn.org/stable/supervised_learning.html).  
- [Kaggle Datasets](https://www.kaggle.com/datasets).  

---

## **D√≠a 4 (8-10 horas) - T√©cnicas Avanzadas en Miner√≠a de Datos y Evaluaci√≥n de Modelos**  
üìå **Objetivo:** Profundizar en **t√©cnicas avanzadas** de miner√≠a de datos y **evaluaci√≥n de modelos**.  

### **Conceptos Clave:**  
‚úÖ Algoritmos avanzados: **XGBoost**, **LightGBM**, **CatBoost**.  
‚úÖ T√©cnicas de **ensembles**: **Bagging**, **Boosting**, **Stacking**.  
‚úÖ T√©cnicas de **overfitting** y **underfitting**.  
‚úÖ Interpretaci√≥n de resultados con **SHAP**, **LIME**, y **feature importance**.  
‚úÖ Evaluaci√≥n avanzada: **Confusion Matrix**, **Precision-Recall Curve**, **Learning Curve**.  

üìå **Pr√°ctica:**  
- Implementar un modelo de **XGBoost** y **LightGBM** para un problema de clasificaci√≥n.  
- Aplicar **Cross-validation** y ajustar modelos para evitar el **overfitting**.  
- Evaluar un modelo con **Confusion Matrix** y **Precision-Recall Curve**.  
- Interpretar un modelo usando **SHAP** o **LIME** para obtener explicaciones sobre predicciones.  

**Recursos:**  
- [XGBoost Docs](https://xgboost.readthedocs.io/).  
- [LightGBM Docs](https://lightgbm.readthedocs.io/).  
- [SHAP Library](https://github.com/slundberg/shap).  

---

## **D√≠a 5 (8-10 horas) - Aplicaci√≥n Pr√°ctica: Proyecto de Miner√≠a de Datos**  
üìå **Objetivo:** Aplicar todo lo aprendido en un **proyecto de miner√≠a de datos** desde la recolecci√≥n hasta la evaluaci√≥n del modelo.  

### **Conceptos Clave:**  
‚úÖ Definir el **problema de negocio**.  
‚úÖ Selecci√≥n y limpieza de **datasets**.  
‚úÖ Desarrollo de **modelos predictivos** y su validaci√≥n.  
‚úÖ Comunicaci√≥n de resultados a trav√©s de **visualizaciones** y **reportes**.  

üìå **Pr√°ctica:**  
- Elegir un conjunto de datos real (de Kaggle o un repositorio p√∫blico).  
- Aplicar t√©cnicas de **preprocesamiento** y **miner√≠a de datos** para resolver un problema de negocio.  
- Desarrollar un **modelo predictivo** y realizar su evaluaci√≥n.  
- Presentar los resultados en un **informe** con visualizaciones y m√©tricas clave.  

**Recursos:**  
- [Kaggle Competitions](https://www.kaggle.com/competitions).  
- [Machine Learning Mastery](https://machinelearningmastery.com/).  

---
